<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.53" />


<title>Everything in statistical modeling can be seen as a regression - Simina blog</title>
<meta property="og:title" content="Everything in statistical modeling can be seen as a regression - Simina blog">



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="https://sites.google.com/georgetown.edu/siminaboca/">Academic site</a></li>
    
    <li><a href="https://github.com/SiminaB/">GitHub</a></li>
    
    <li><a href="https://icbi.georgetown.edu/">ICBI</a></li>
    
    <li><a href="https://healthinformatics.georgetown.edu/">MS in HIDS</a></li>
    
    <li><a href="https://twitter.com/siminaboca">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">12 min read</span>
    

    <h1 class="article-title">Everything in statistical modeling can be seen as a regression</h1>

    
    <span class="article-date">2018/01/10</span>
    

    <div class="article-content">
      


<p>This is of course a bit of an exaggeration! It also depends on the way things are emphasized in, for example, statistical
graduate programs. I know that my own training program was highly focused on seeing much of statistics as either a
special case or a generalization of some kind of linear regression model. I kind of love this view and try to emphasize it whenever I can
to collaborators, although I do admit that sometimes you have to start from special cases in order to build up the whole.</p>
<div id="regression-concepts-connected-to-loss-functions-and-conditional-means-and-medians" class="section level2">
<h2>Regression concepts connected to loss functions and conditional means and medians</h2>
<p>The term “regression” is currently used in a large variety of ways. In general, if there a variable <span class="math inline">\(Y\)</span>, called the outcome or dependent variable, and some other variables <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span> etc, called the explanatory or independent variables, then a regression of <span class="math inline">\(Y\)</span> on the <span class="math inline">\(X\)</span>s can be <a href="http://www.jstor.org/stable/2727353">‘any feature of the probability distribution" of <span class="math inline">\(Y\)</span> given (or “conditional on”) the <span class="math inline">\(X\)</span>.’</a> These features often relate to some measure of “average” or “center” of the distribution, such as the mean or the median.</p>
<p>The simplest case of regression is that of linear regression with a single explanatory variable. In other words, trying to
draw a “best fit line” through a scatterplot. There are many ways to define the “best fit” and the one that is usually
considered is based on the <a href="https://en.wikipedia.org/wiki/Loss_function#Quadratic_loss_function">quadratic loss function</a>
(also known as squared error loss or L<sub>2</sub> loss.)
Basically, if one seeks to <em>minimize the sum of the squares of the errors</em>, i.e. the differences between the true outcome Y and the fitted values on the
line <span class="math inline">\(\hat{Y}\)</span>, the solution is the “usual” <a href="https://en.wikipedia.org/wiki/Ordinary_least_squares">ordinary least squares (OLS)</a>.
Take this example where there is a single explanatory variable <span class="math inline">\(X_1\)</span> and the outcome variable <span class="math inline">\(Y\)</span> simply adds some noise. The regression OLS fit is in blue, as are the segments between fitted values and the true values of Y. Two other lines are shown in dashed orange.
<img src="/post/2018-01-10-Regression_files/figure-html/unnamed-chunk-1-1.svg" width="576" /></p>
<p>The sum of the squared errors for the OLS fit is 6.19, while for the other two lines
it is 7.58 and
10.19.</p>
<p>If there is no explanatory variable, the fitted line is flat and represents the average (mean) value of the outcome
values:
<img src="/post/2018-01-10-Regression_files/figure-html/unnamed-chunk-2-1.svg" width="576" /></p>
<p>For the example above, this is clearly a poor fit. In fact, the sum of the squared errors in this case is 49.34!</p>
<p>In general, the OLS has some nice properties, for example if the noise terms - defined as <span class="math inline">\(\epsilon = Y - \beta_0 - \beta_1 X_1\)</span>, where
<span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are the true intercept and slope of the line - are independent and identically distributed with finite
variance, the OLS fit is an unbiased estimator of the conditional mean E(Y|X) (i.e. its mean is equal to the conditional
mean) and is in fact the <a href="https://en.wikipedia.org/wiki/Minimum-variance_unbiased_estimator">minimum-variance unbiased estimator</a>.
Note that so far I haven’t discussed the distribution of Y or even whether Y is continuous. However, in the case where the noise terms
(and therefore Y conditional on X) are normally distributed, the OLS is also the maximum likelihood estimator (MLE) of the conditional mean
E(Y|X).</p>
<p>One reason why the quadratic loss function is considered very often is because the mathematics works out very nicely (can solve for the maximum using the usual calculus-based second
derivative test). This also generalizes nicely if there are more explanatory variables - we can just include them all in a matrix
X and essentially proceed in the same way with the help of some multivariate calculus.
There are however many other ways of defining the “best fit.”
However, perhaps one is interested in
estimating an aspect of the distribution of Y|X other than its mean, such as its median.
The median is naturally connected to the <a href="https://en.wikipedia.org/wiki/Loss_function">absolute loss</a>
(also known as as the L<sub>1</sub> loss) in the same way that the mean is connected to the
quadratic error loss. In this case, the criterion is to <em>minimize the sum of the absolute values of the errors</em>,
also called the
<a href="https://en.wikipedia.org/wiki/Least_absolute_deviations">least absolute deviations (LAD) or least absolute residuals</a>.
In the absence of an explanatory variable, the flat fitted line represents the median of the explanatory values.
If we look more generally for a function f where the values f(X) are most similar to Y, with the similarity being
defined using the quadratic loss, then the “best fit” (which minimizes
<a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf">the expected prediction error (EPE - see <em>Elements of statistical learning</em>, section 2.4)</a> is the function
f(x) = E(Y|X=x) (conditional mean); by contrast, if it is defined using the absolute loss, it is
f(x) = median(Y|X=x) (conditional median). In general, medians are more robust than means, given their lack of dependence
on outliers, but the math gets harder much faster due to discontinuous derivatives which complicate optimization procedures. For the example above, the results are almost identical whether minimizing the sum of squares or the sum of absolute values of the errors:</p>
<pre class="r"><code>##intercept and slope estimated via OLS
coef(lm(y ~ x1))</code></pre>
<pre><code>## (Intercept)          x1 
##   0.1263162   0.9645116</code></pre>
<pre class="r"><code>library(quantreg)
##intercept and slope estimated via quantile regression with median (see below)
coef(rq(y ~ x1, tau=0.5))</code></pre>
<pre><code>## (Intercept)          x1 
##   0.1218339   0.9931817</code></pre>
<p>Similarly, if a <a href="http://www.jstor.org/stable/2727353">step function (0-1) loss</a> is used instead,
the result is f(x) = mode(Y|X=x) (conditional mode). Note that the mean, median, and mode
are the most common ways of describing the “center” of a continuous distribution.</p>
<p>It may also be of interest to consider another quantile besides the median, which is sometimes the case
when considering a variable with an asymmetric distribution, such as income distributions.
<a href="https://en.wikipedia.org/wiki/Quantile_regression">Quantile regression</a> considers different loss functions
for this purpose; the loss function used for the median is however equivalent to the LAD. If the noise terms have an asymmetric Laplace distribution, the estimator obtained here for a specific quantile is
its <a href="http://www.american.edu/cas/economics/info-metrics/pdf/upload/working-paper-bera.pdf">MLE</a>.
For the rest of this document, we will generally use “regression” in the “usual way,” considering the square error loss.</p>
</div>
<div id="more-about-the-ols-fit-linear-algebra-and-geometric-interpretation" class="section level2">
<h2>More about the OLS fit: Linear algebra and geometric interpretation</h2>
<p>Above we considered the special case of fitting lines in 2 dimensions, where there was a single explanatory variable and two parameters were estimated: the slope and the intercept of the “best fitting line.” In general, many variables can be considered. It is common to consider all these variables in a single matrix <span class="math inline">\(X\)</span>, which has each variable as a column and each sample as a row, with the first column typically consisting of 1s. The goal is then to estimate a vector of “parameters” <span class="math inline">\(\beta\)</span>, so that the outcome <span class="math inline">\(Y\)</span> is “as close as possible” to <span class="math inline">\(X\beta\)</span> (thus, the first element of <span class="math inline">\(\beta\)</span> is the intercept). Under certain linear algebra assumptions, there is a nice <a href="https://en.wikipedia.org/wiki/Proofs_involving_ordinary_least_squares">closed form for the OLS in the multivariate case</a>:
<span class="math inline">\(\hat{\beta} = (X&#39;X)^{-1}X&#39;Y\)</span>.
This means that the fitted values are:
<span class="math inline">\(\hat{Y}=X\hat{\beta} = X(X&#39;X)^{-1}X&#39;Y\)</span>.
The matrix <span class="math inline">\(X(X&#39;X)^{-1}X&#39;\)</span> is in fact the <a href="https://en.wikipedia.org/wiki/Ordinary_least_squares">projection matrix onto the space spanned by the columns of X</a>, which is another way of conceptualizing that <span class="math inline">\(\hat{Y}\)</span> represents the closest vector to <span class="math inline">\(Y\)</span> out of all the vectors that are linear combinations of the explanatory variables. Once again, we define “closest” using the <span class="math inline">\(L^2\)</span> loss, or, in linear algebra terms, minimize the <span class="math inline">\(L^2\)</span> norm <span class="math inline">\(||.||\)</span>:
<span class="math inline">\(\hat{\beta} = \arg\min||Y-X\beta||\)</span>.
It is also interesting to note that solving for <span class="math inline">\(\beta\)</span> in <span class="math inline">\(X\beta = Y\)</span> would in fact represent an <a href="https://en.wikipedia.org/wiki/Ordinary_least_squares">overdetermined system</a> and as such cannot usually be solved exactly - we thus look for a solutions that gives a fit that is “close” to Y while at the same time being in the space spanned by the explanatory variables.</p>
</div>
<div id="t-tests-anova-and-all-the-rest-hypothesis-testing-in-linear-models" class="section level2">
<h2>T-tests, ANOVA, and all the rest: Hypothesis testing in linear models</h2>
<p>Perhaps the first thing most people think of when they think statistics is the t-test. <a href="https://en.wikipedia.org/wiki/Student%27s_t-test#Independent_two-sample_t-test">Student’s two-sample t-test</a> is used to compare the means of two distributions, with the null hypothesis stating that they are equal, for either normally distributed random variables or for continuous random variables with a large enough sample size. One of the powerful things about linear regression is that a two-sample t-test which assumes equal variances is in fact the same as a linear model where the explanatory variable is a 0/1 variable which defines group membership:</p>
<pre class="r"><code>set.seed(380148)
n &lt;- 10
##define the two random variables
##y1 is from a normal distribution with mean=0, sd=1
y1 &lt;- rnorm(n,0,1)
##y2 is from a normal distribution with mean=0.5, sd=1
y2 &lt;- rnorm(n,0.5,1)
##run a t-test!
t.test(y1,y2,var.equal = TRUE)</code></pre>
<pre><code>## 
##  Two Sample t-test
## 
## data:  y1 and y2
## t = -0.66034, df = 18, p-value = 0.5174
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -1.5638050  0.8158546
## sample estimates:
## mean of x mean of y 
## 0.2961889 0.6701641</code></pre>
<pre class="r"><code>##create a single vector of outcomes and a vector indicating group membership
y &lt;- c(y1,y2)
x1 &lt;- c(rep(0,length(y1)),rep(1,length(y2)))
summary(lm(y~x1))</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x1)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.3130 -0.9469  0.1265  1.0192  1.7432 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)   0.2962     0.4005    0.74    0.469
## x1            0.3740     0.5663    0.66    0.517
## 
## Residual standard error: 1.266 on 18 degrees of freedom
## Multiple R-squared:  0.02365,    Adjusted R-squared:  -0.03059 
## F-statistic: 0.436 on 1 and 18 DF,  p-value: 0.5174</code></pre>
<p>As we can see, the p-value is the same and the estimated slope is in fact equal to the differences between means. Since one of the assumptions behind the properties of the OLS is that the noise terms are identically distributed, this means that the case of unequal variances cannot be directly placed in this framework. However, if the variances are unequal, this means that perhaps other explanatory variables should be considered, so these could then be included in a regression model.</p>
<p>The same idea holds in the case of ANOVA, where multiple group means are being compared. One of the benefits of having a regression mindset is that it very easy to interpret the results in terms of conditional means and it also allows for a unified framework which can include both continuous and discrete explanatory variables and interactions between them, without having to resort to special terminology like two-way ANOVA, ANCOVA, etc.</p>
<p>In general, we can test whether any linear combination of parameters in a multivariate model is equal to 0 by performing an <a href="https://courses.washington.edu/b515/l6.pdf">F-test</a>. This is a powerful approach, as it includes cases including the equality of two coefficients and of course, has a special case whether a single parameter is equal to 0.
For this latter case, this approach is equivalent to using a t-test with <span class="math inline">\(n-p-1\)</span> degrees of freedom, where <span class="math inline">\(p\)</span> is the number of parameters, excluding the intercept. Thus, once again, the regression approach allows for a more general framework from which the special cases naturally fall out.</p>
</div>
<div id="from-weighted-regression-to-generalized-linear-models" class="section level2">
<h2>From weighted regression to generalized linear models</h2>
<p>We mentioned before that one of the properties that is generally assumed for linear regression is that the noise terms are independent and identically distributed. In case this does not hold, an <a href="https://en.wikipedia.org/wiki/Generalized_linear_model">alterative to OLS</a> is to use either <em>weighted least squares</em> (WLS) or <em>generalized least squares</em> (GLS) approaches. This means that if the variance-covariance matrix of the noise terms is <span class="math inline">\(W\)</span>,
then the MLE solution is:
<span class="math inline">\(\hat{\beta}=(X&#39;W^{-1}X)^{-1}X&#39;W^{-1}Y\)</span>.
This is equivalent to minimizing
<span class="math inline">\(\hat{\beta} = \arg\min(Y-X\beta)&#39;W^{-1}(Y-X\beta)\)</span>, known as the <em>Mahalanobis length,</em>
instead of
<span class="math inline">\(\hat{\beta} = \arg\min||Y-X\beta||\)</span> as before.
If W has all off-diagonal terms equal to 0 (i.e. the noise terms are independent), this is the WLS estimate, otherwise it is the GLS estimate.</p>
<p>What happens if the outcome is not normally distributed?
Here we come to the <a href="https://en.wikipedia.org/wiki/Generalized_linear_model">generalized linear model</a>.
In the case in which the outcome Y is from a class of distributions known as the
<a href="https://en.wikipedia.org/wiki/Exponential_family">exponential family</a> - which include the normal, exponential, and Bernoulli distributions -
instead of trying to directly estimate the conditional mean <span class="math inline">\(E(Y|X)\)</span> as a linear function <span class="math inline">\(X\beta\)</span>, it is generally more convenient to estimate a transformation of this mean, <span class="math inline">\(g[E(Y|X)]\)</span>. This is because, for example, for a 0/1 outcome, using just the OLS will often result in fitted values outside of the [0,1] range, but using a <em>link function</em> g that transforms the (0,1) range corresponding to the probability of success in a Bernoulli distribution into the entire real line solves this problem. If the link function is the log-odds (logit), we find ourselves looking at logistic regression.
In the general case, we cannot obtain a closed form MLE solution, but by applying a first-order Taylor approximation, we obtain an algorithm that performs <a href="https://web.as.uky.edu/statistics/users/pbreheny/760/S13/notes/2-19.pdf">iteratively reweighted least squares (IRLS)</a>, where the matrix <span class="math inline">\(W\)</span> is related to the derivative of the inverse link function.</p>
</div>
<div id="smoothing-and-regression" class="section level2">
<h2>Smoothing and regression</h2>
<p>So far we have discussed fitting linear models between an outcome and explanatory variables, but what happens if the relationship is clearly nonlinear? One option is to add more explanatory variables, including polynomial terms, and fit a linear function to those - for example, instead of having <span class="math inline">\(X_1\)</span> as an explanatory variable, can consider <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_1^2\)</span>, so that we fit the model <span class="math inline">\(E(Y|X_1) = \beta_0+\beta_1X_1+\beta_2X_1^2\)</span> instead of <span class="math inline">\(E(Y|X_1) = \beta_0+\beta_1X_1\)</span>. Note that the function is still linear in the explanatory variables that are considered. More flexibility can be obtained by using regression models
that have piecewise polynomial terms, known as <a href="https://en.wikipedia.org/wiki/Spline_(mathematics)">splines</a>.
For the case of a single explanatory variable, another popular approach is <a href="https://en.wikipedia.org/wiki/Local_regression">local regression (loess)</a>, which fits simple linear regression using a moving window, resulting in a smooth function.
Many other advanced approaches are built on these concepts, including <a href="https://en.wikipedia.org/wiki/Smoothing_spline">smoothing splines</a>, which place knots at every observation but have a <a href="https://web.stanford.edu/class/stats202/content/lec17.pdf">“roughness penalty” to reduce overfitting</a>
and <a href="https://web.stanford.edu/class/stats202/content/lec17.pdf">generalized additive models (GAMs)</a> which fit smooth functions for each separate explanatory variable.</p>
</div>
<div id="regression-and-machine-learning" class="section level2">
<h2>Regression and machine learning</h2>
<p><em>Machine learning</em>, also known as <em>statistical learning</em>, focuses on “learning from the data” in order to either predict the outcomes for new data points (supervised learning) or to find patterns in the data (unsupervised learning). “Predicting” in this case means obtaining the values <span class="math inline">\(\hat{Y}\)</span> for new points which are not in the dataset on which the model was fitted (the “training data”), using the model parameters estimated from that dataset.
For supervised learning tasks, any of the regression approaches described above can be considered, noting that in machine learning, the main goal is having a low prediction error for new points, as opposed to performing statistical inference. If the number of predictors is very large, potentially larger than the number of samples, <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">regularization approaches</a> like ridge regression, LASSO, or elastic net can be used to reduce overfitting.
Many more approaches exist, some which build on or are similar to regression. For example, <a href="https://en.wikipedia.org/wiki/Support_vector_machine#Regression">support vector machines (SVMs)</a> for continuous outcomes are equivalent to regression models with a regularization term and the hinge loss instead of the quadratic loss function. Similarly, <a href="https://github.com/greenelab/deep-review/blob/master/content/02.intro.md">neural networks</a> can be seen as extensions of linear or logistic regression. It is also increasingly common in machine learning to use <a href="https://en.wikipedia.org/wiki/Ensemble_learning">ensemble methods</a> that combine several approaches and perform a type of averaging or “voting” to obtain the final predicted values.</p>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

